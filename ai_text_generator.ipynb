{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook: AI Text Generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Introduction: Text Generation using Neural Networks**\n",
    "\n",
    "In this project, we explore the realm of neural networks and their application in generating text. The primary goal was to create a model capable of generating coherent and contextually relevant phrases, building upon the patterns found in a given dataset (here, a poem about cookies).\n",
    "\n",
    "**2. Libraries and Technologies Used:**\n",
    "\n",
    "The project leverages the power of Python and several key libraries to implement and train the neural network. Notably, it employs NumPy for numerical operations, and PyTorch for building and training the neural network.\n",
    "\n",
    "**3. Project Overview:**\n",
    "\n",
    "The initial step involved preprocessing a text dataset by converting it to lowercase, extracting unique characters, and assigning numerical values to each character. This laid the foundation for transforming textual information into a format suitable for neural network training.\n",
    "\n",
    "The model architecture consists of an ensemble of four attention-based heads, each contributing to the learning process. These heads focus on capturing different aspects of the input sequence. The model then progresses through layers of fully connected neural networks, ultimately generating a prediction for the next character in the sequence.\n",
    "\n",
    "**4. Training and Optimization:**\n",
    "\n",
    "The training process involves iteratively feeding sequences of characters into the model, optimizing the model parameters with respect to the desired predictions. The loss function used for optimization is cross-entropy, a suitable choice for sequence generation tasks.\n",
    "\n",
    "An Adam optimizer is employed to efficiently update the weights of the model during training. The training loop runs for a predefined number of iterations, with periodic updates on the loss for monitoring the model's progress.\n",
    "\n",
    "**5. Results:**\n",
    "\n",
    "As the model evolves through training iterations, it gains the ability to generate new text sequences. The project concludes with a demonstration of the model's capabilities by generating a text sequence, showcasing the potential for creative and contextually relevant outputs.\n",
    "\n",
    "For a detailed, step-by-step explanation of the code, see neural_network.ipynb file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 3.3980443477630615\n",
      "1000 loss: 1.3685879707336426\n",
      "2000 loss: 0.7451860308647156\n",
      "3000 loss: 0.5296571254730225\n",
      "4000 loss: 0.45431315898895264\n",
      "generated text:\n",
      "---------------------\n",
      " bite of this coon supreme, base.\n",
      "vanilla dreams, a warming hue,\n",
      "a symphony, a sweet little win.\n",
      "\n",
      "oa\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# To open a text file, run the following:\n",
    "# with open('file.txt'. 'r'. encoding='utf-8') as f:\n",
    "#     text = f.read()\n",
    "\n",
    "text = '''\n",
    "\n",
    "In the oven's warm embrace they lie,\n",
    "A symphony of scents begins to fly.\n",
    "Buttery whispers, sugary notes,\n",
    "A dance of flavors, the cookie gloats.\n",
    "\n",
    "Golden brown, a caramel delight,\n",
    "A treat to savor, a delicious bite.\n",
    "Chocolate chips, like stars in the night,\n",
    "Melting away, a sweet delight.\n",
    "\n",
    "Cinnamon swirls in a fragrant breeze,\n",
    "A cookie masterpiece, sure to please.\n",
    "Soft and chewy, or crisp and thin,\n",
    "Each creation a sweet little win.\n",
    "\n",
    "Oatmeal clusters, a hearty blend,\n",
    "A journey of taste that has no end.\n",
    "Almond accents, nutty and bold,\n",
    "In every nibble, a story told.\n",
    "\n",
    "Sugar dusted, a powdered grace,\n",
    "A sprinkle of joy on a sugary base.\n",
    "Vanilla dreams, a comforting hug,\n",
    "Wrapped in dough, like a cozy rug.\n",
    "\n",
    "So, take a bite of this cookie song,\n",
    "A delightful chorus that won't be long.\n",
    "In the world of treats, they reign supreme,\n",
    "Oh, sweet cookies, a heavenly dream.\n",
    "\n",
    "But wait, there's more in the cookie jar,\n",
    "A symphony of flavors, near and far.\n",
    "Macadamia melodies, a crunchy delight,\n",
    "Mingling with coconut, oh, what a sight.\n",
    "\n",
    "Peanut butter echoes, smooth and rich,\n",
    "A delectable chorus, an irresistible pitch.\n",
    "Raisin rhythms, a sweet surprise,\n",
    "In the cookie dance, they harmonize.\n",
    "\n",
    "Lemon zest, a citrusy kiss,\n",
    "Adds a zing to the cookie bliss.\n",
    "Ginger sparks, a warming hue,\n",
    "A flavor adventure, both old and new.\n",
    "\n",
    "Minty freshness, a cool refrain,\n",
    "Peppermint pleasures, a candy cane.\n",
    "Maple whispers, autumn's embrace,\n",
    "In the cookie kingdom, a royal grace.\n",
    "\n",
    "Butterscotch tales, a caramel swirl,\n",
    "A caramel symphony, a tempting twirl.\n",
    "Hazelnut dreams, a nutty trance,\n",
    "In the world of cookies, a sweet advance.\n",
    "\n",
    "So, as you explore this cookie sea,\n",
    "Let your taste buds wander, wild and free.\n",
    "In every crumb and every seam,\n",
    "A cookie world, a delicious dream.\n",
    "\n",
    "'''\n",
    "\n",
    "# Preprocessing, change to lower case to reduce vocab size\n",
    "text = text.lower()\n",
    "# Get letters\n",
    "chars = sorted(list(set(text)))\n",
    "# Assign a number to each letter in a dictionary\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "# Convert text to numbers\n",
    "data = [stoi[c] for c in text]\n",
    "vocab_size = len(chars)\n",
    "\n",
    "ins = 16\n",
    "outs = vocab_size\n",
    "nodes = 100\n",
    "lr = 0.001\n",
    "\n",
    "n_emb = 32\n",
    "embed = torch.randn(vocab_size, n_emb)\n",
    "pos = torch.randn(ins, n_emb)\n",
    "\n",
    "data = torch.tensor(data).long()\n",
    "\n",
    "\n",
    "def generate_text(s, length):\n",
    "    print(\"generated text:\")\n",
    "    print(\"---------------------\")\n",
    "    gen_text = \"\"\n",
    "    for i in range(length):\n",
    "        yh = model.forward(s)\n",
    "        prob = F.softmax(yh[-1, :], dim=0)\n",
    "        # pred = torch.argmax(yh).item()\n",
    "        pred = torch.multinomial(prob, num_samples=1).item()        \n",
    "        s = torch.roll(s, -1)\n",
    "        s[-1] = pred\n",
    "        gen_text += itos[pred]\n",
    "    \n",
    "    print(gen_text)\n",
    "\n",
    "params = [] \n",
    "def weights(ins, outs):\n",
    "    ws = torch.randn(ins, outs)*0.1\n",
    "    ws.requires_grad_(True)\n",
    "    params.append(ws)\n",
    "    return ws\n",
    "\n",
    "class Head():\n",
    "    def __init__(self):\n",
    "        self.wv = weights(n_emb, n_emb//4)\n",
    "        self.wr = weights(n_emb, ins)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        v = x @ self.wv\n",
    "        attn = x @ self.wr\n",
    "        tril = torch.tril(attn)\n",
    "        tril = tril.masked_fill(tril == 0, -1e10)\n",
    "        rew = F.softmax(tril, dim=-1)\n",
    "        x = rew @ v\n",
    "        return x        \n",
    "\n",
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.heads = [Head(), Head(), Head(), Head()]\n",
    "        self.w0 = weights(n_emb, nodes)\n",
    "        self.w1 = weights(nodes, nodes)\n",
    "        self.w2 = weights(nodes, outs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = embed[x] + pos\n",
    "        x = torch.cat([head.forward(x) for head in self.heads], dim=-1)        \n",
    "        x = torch.relu(x @ self.w0)\n",
    "        x = torch.relu(x @ self.w1)\n",
    "        yh = x @ self.w2    \n",
    "        return yh\n",
    "    \n",
    "model = Model()\n",
    "optimizer = torch.optim.Adam(params, lr)\n",
    "\n",
    "for it in range(5000):\n",
    "    \n",
    "    b = torch.randint(len(data)-ins, (100,))\n",
    "    xs = torch.stack([data[i:i+ins] for i in b])\n",
    "    ys = torch.stack([data[i+1:i+ins+1] for i in b])\n",
    "\n",
    "    yh = model.forward(xs)\n",
    "    loss = F.cross_entropy(yh.view(-1, vocab_size) , ys.long().view(-1)) \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    e = loss.item()\n",
    "    if it%1000==0:\n",
    "        print(it, \"loss:\", e)\n",
    "            \n",
    "generate_text(xs[0], 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
